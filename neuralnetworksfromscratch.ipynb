{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Essentiel pour les array, calculs, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt  # Utilisé pour les graphiques et les images\n",
    "\n",
    "from scipy import signal  # Calcul de convolution : le coder soi-même est possible\n",
    "# Les boucles for sont très coûteuses en termes de performance\n",
    "# et n'optimisent pas les calculs (réseau trop lent)\n",
    "\n",
    "from skimage.measure import block_reduce  # Calcul de maxpooling (même raison que la convolution)\n",
    "# Nécessaire pour la rétropropagation, mais il n'existe pas de fonction\n",
    "# intégrée pour cette tâche, donc on utilise des boucles.\n",
    "# Cela rend le programme plus lent.\n",
    "\n",
    "import time  # Utilisé pour chronométrer les performances du programme\n",
    "\n",
    "import scipy  # Importé pour la fonction scipy.special.expit, qui permet de\n",
    "# calculer la fonction sigmoid sans overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Implémentation du réseau de neurones\n",
    "\n",
    "## 1.1 Couches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    \n",
    "    def __init__(self, taille_entree, taille_sortie):\n",
    "        self.poids = np.random.randn(taille_sortie, taille_entree)\n",
    "        self.biais = np.random.randn(taille_sortie, 1)\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        return (np.dot(self.poids, self.entree) + self.biais)\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        grad_poids = np.dot(grad_sortie, self.entree.T)\n",
    "        \n",
    "        self.poids -= pas_apprentissage * grad_poids\n",
    "        self.biais -= pas_apprentissage * grad_sortie\n",
    "        return np.dot(self.poids.T, grad_sortie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution():\n",
    "    \n",
    "    def __init__(self, dimensions_entree, taille_filtre, profondeur_sortie):  \n",
    "        # profondeur_sortie = nombre de filtres\n",
    "        self.profondeur_sortie = profondeur_sortie\n",
    "        self.profondeur_entree, self.hauteur_entree, self.largeur_entree = dimensions_entree\n",
    "        self.dimensions_sortie = (profondeur_sortie, self.hauteur_entree - taille_filtre + 1, self.largeur_entree - taille_filtre + 1)\n",
    "        self.dimensions_filtres = (profondeur_sortie, self.profondeur_entree, taille_filtre, taille_filtre)\n",
    "        self.filtres = np.random.randn(*self.dimensions_filtres)\n",
    "        self.biais = np.random.randn(*self.dimensions_sortie)\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        self.sortie = np.copy(self.biais)\n",
    "        for i in range(self.profondeur_sortie):\n",
    "            for j in range(self.profondeur_entree):\n",
    "                self.sortie[i] += signal.correlate2d(self.entree[j], self.filtres[i,j], \"valid\")\n",
    "        return self.sortie\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        grad_filtres = np.zeros(self.dimensions_filtres)\n",
    "        grad_entree = np.zeros(self.dimensions_entree)\n",
    "        \n",
    "        for i in range(self.profondeur_sortie):\n",
    "            for j in range(self.profondeur_entree):\n",
    "                grad_filtres[i,j] = signal.correlate2d(self.entree[j], grad_sortie[i], \"valid\")\n",
    "                grad_entree[j] += signal.correlate2d(grad_sortie[i], self.filtres[i,j], \"full\")\n",
    "        \n",
    "        grad_biais = grad_sortie\n",
    "        \n",
    "        # Mise à jour\n",
    "        self.filtres -= pas_apprentissage * grad_filtres\n",
    "        self.biais -= pas_apprentissage * grad_biais\n",
    "        \n",
    "        return grad_entree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpooling():\n",
    "    \n",
    "    def __init__(self, dimensions_entree, taille_filtre, stride):\n",
    "        self.profondeur_entree, self.hauteur_entree, self.largeur_entree = dimensions_entree\n",
    "        self.dimensions_entree = dimensions_entree\n",
    "        self.taille_filtre = taille_filtre\n",
    "        self.filtre_h, self.filtre_l = taille_filtre, taille_filtre\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.hauteur_sortie = int(1 + (self.hauteur_entree - self.filtre_h) / stride)\n",
    "        self.largeur_sortie = int(1 + (self.largeur_entree - self.filtre_l) / stride)\n",
    "        self.profondeur_sortie = self.profondeur_entree\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        sortie = np.zeros((self.profondeur_sortie, self.hauteur_sortie, self.largeur_sortie))\n",
    "        stride = self.stride\n",
    "        \n",
    "        for c in range(self.profondeur_sortie):\n",
    "            for i in range(self.hauteur_sortie):\n",
    "                for j in range(self.largeur_sortie):\n",
    "                    sortie[c, i, j] = np.max(entree[c, i * stride : i * stride + self.filtre_h, j * stride : j * stride + self.filtre_l ])\n",
    "        return sortie\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        grad_entree = np.zeros((self.profondeur_entree, self.hauteur_entree, self.largeur_entree))\n",
    "        entree = self.entree\n",
    "        stride = self.stride\n",
    "        \n",
    "        for c in range(self.profondeur_sortie):\n",
    "            for i in range(self.hauteur_sortie):\n",
    "                for j in range(self.largeur_sortie):\n",
    "                    intermediaire = entree[c, i * stride : i * stride + self.filtre_h, j * stride : j * stride + self.filtre_l]\n",
    "                    i_max, j_max = np.where(np.max(intermediaire) == intermediaire)\n",
    "                    i_max, j_max = i_max[0], j_max[0]\n",
    "                    grad_entree[c, i * stride : i * stride + self.filtre_h, j * stride + self.filtre_l][i_max, j_max] = grad_sortie[c, i, j]\n",
    "        \n",
    "        return grad_entree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    \n",
    "    def __init__(self, q_bernouilli):\n",
    "        self.p_bernouilli = 1 - q_bernouilli\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        self.masque_binaire = np.random.binomial(1, self.p_bernouilli, size=entree.shape) / self.p_bernouilli\n",
    "        self.sortie = entree * self.masque_binaire\n",
    "        return self.sortie\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        return grad_sortie * self.masque_binaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dimension():\n",
    "    \n",
    "    def __init__(self, dimension_entree, dimension_sortie):\n",
    "        self.dimension_entree = dimension_entree\n",
    "        self.dimension_sortie = dimension_sortie\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        return np.reshape(entree, self.dimension_sortie)\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        return np.reshape(grad_sortie, self.dimension_entree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Couches d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangente hyperbolique\n",
    "class Tanh():\n",
    "    \n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_p = lambda x: 1 - np.tanh(x)**2\n",
    "        self.activation = tanh\n",
    "        self.derivee_activation = tanh_p\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        return self.activation(self.entree)\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        return np.multiply(grad_sortie, self.derivee_activation(self.entree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoide\n",
    "class Sigmoide():\n",
    "    \n",
    "    def __init__(self):\n",
    "        def sigmoide(x):\n",
    "            return scipy.special.expit(x)\n",
    "        \n",
    "        def sigmoide_p(sig):\n",
    "            return sig * (1 - sig)\n",
    "        \n",
    "        self.activation = sigmoide\n",
    "        self.derivee_activation = sigmoide_p\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        self.sig = self.activation(self.entree)\n",
    "        return self.sig\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        return np.multiply(grad_sortie, self.derivee_activation(self.sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "class Softmax():\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        maxi = np.max(entree)\n",
    "        entree = entree - maxi\n",
    "        expo = np.exp(entree)\n",
    "        self.sortie = expo / np.sum(expo)\n",
    "        return self.sortie\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        # L'erreur utilisée étant toujours cce\n",
    "        # en complément de softmax, et comme on a déjà calculé la dérivée de \n",
    "        # l'erreur par rapport à l'entrée du softmax, on la transmet (cf cce)\n",
    "        return grad_sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class Relu():\n",
    "    \n",
    "    def propagation_directe(self, entree):\n",
    "        self.entree = entree\n",
    "        self.sortie = np.maximum(0, entree)\n",
    "        return self.sortie\n",
    "    \n",
    "    def retropropagation(self, grad_sortie, pas_apprentissage):\n",
    "        inter = grad_sortie.copy()\n",
    "        inter[inter <= 0] = 0\n",
    "        return inter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur quadratique moyenne (eqm)\n",
    "def eqm(sortie_voulue, sortie):\n",
    "    return np.mean(np.power(sortie_voulue - sortie, 2))\n",
    "\n",
    "def eqm_derivee(sortie_voulue, sortie):\n",
    "    return 2 * (sortie - sortie_voulue) / np.size(sortie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur croisée binaire (binary crossentropy notée bce)\n",
    "def bce(sortie_voulue, sortie):\n",
    "    sortie_voulue = np.clip(sortie_voulue, 1e-7, 1 - 1e-7)  # Pour éviter les divisions par zero/log(0)\n",
    "    sortie = np.clip(sortie, 1e-7, 1 - 1e-7)  # Pour éviter les divisions par zero/log(0)\n",
    "    return -np.mean(sortie_voulue * np.log(sortie) + (1 - sortie_voulue) * np.log(1 - sortie))\n",
    "\n",
    "def bce_derivee(sortie_voulue, sortie):\n",
    "    sortie = np.clip(sortie, 1e-7, 1 - 1e-7)\n",
    "    sortie_voulue = np.clip(sortie_voulue, 1e-7, 1 - 1e-7)\n",
    "    return ((1 - sortie_voulue) / (1 - sortie) - sortie_voulue / sortie) / np.size(sortie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur croisée (categorical crossentropy notée cce)\n",
    "def cce(sortie_voulue, sortie):\n",
    "    sortie_voulue = np.clip(sortie_voulue, 1e-7, 1 - 1e-7)\n",
    "    sortie = np.clip(sortie, 1e-7, 1 - 1e-7)\n",
    "    return -np.sum(np.log(sortie) * sortie_voulue)\n",
    "\n",
    "def cce_derivee(sortie_voulue, sortie):\n",
    "    # Ici, pour un souci de rapidité de calcul, on calcule directement la dérivée de l'erreur par rapport à l'entrée du softmax\n",
    "    # en utilisant cce comme erreur à la dernière couche.\n",
    "    sortie_voulue = np.clip(sortie_voulue, 1e-7, 1 - 1e-7)\n",
    "    sortie = np.clip(sortie, 1e-7, 1 - 1e-7)\n",
    "    return sortie - sortie_voulue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Réseaux de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_erreur(res, entree_t, sortie_t):\n",
    "    succes = 0\n",
    "    total = 0\n",
    "    e = 0\n",
    "    \n",
    "    for i in range(len(entree_t)):\n",
    "        s = res.prediction(entree_t[i])\n",
    "        e += res.erreur(sortie_t[i], s)\n",
    "        maxi = np.argmax(s)\n",
    "        if maxi == np.argmax(sortie_t[i]):\n",
    "            succes += 1\n",
    "        total += 1\n",
    "    \n",
    "    return (succes / total, e / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reseau():\n",
    "    \n",
    "    def __init__(self, couches, erreur, erreur_derivee):\n",
    "        self.couches = couches\n",
    "        self.erreur = erreur\n",
    "        self.erreur_derivee = erreur_derivee\n",
    "    \n",
    "    def prediction(self, entree):\n",
    "        sortie = entree\n",
    "        for couche in self.couches:\n",
    "            sortie = couche.propagation_directe(sortie)\n",
    "        return sortie\n",
    "    \n",
    "    def entrainement(self, entree_e, sortie_e, entree_t, sortie_t, iterations, pas_apprentissage):\n",
    "        # entree_e et sortie_e : entrée et sortie entraînement\n",
    "        nb_entrainements = len(entree_e)\n",
    "        liste_erreur = []\n",
    "        liste_erreur_t = []\n",
    "        precision_e = []\n",
    "        precision_t = []\n",
    "        tini = time.time()\n",
    "\n",
    "        for itera in range(iterations):\n",
    "            print(\"Itération numéro \", itera+1)\n",
    "            erreur = 0\n",
    "            titer = time.time()\n",
    "            succes_e = 0\n",
    "            tot_e = 0\n",
    "            \n",
    "            for i in range(nb_entrainements):\n",
    "                # Propagation directe\n",
    "                sortie = entree_e[i]\n",
    "                for couche in self.couches:\n",
    "                    sortie = couche.propagation_directe(sortie)\n",
    "                if np.argmax(sortie) == np.argmax(sortie_e[i]):\n",
    "                    succes_e += 1\n",
    "                tot_e += 1\n",
    "\n",
    "                # Ajout de l'erreur\n",
    "                erreur += self.erreur(sortie_e[i], sortie)\n",
    "                \n",
    "                # Rétropropagation\n",
    "                sortie_retro = self.erreur_derivee(sortie_e[i], sortie)\n",
    "                for couche in reversed(self.couches):\n",
    "                    sortie_retro = couche.retropropagation(sortie_retro, pas_apprentissage)\n",
    "\n",
    "            # Traitement de l'erreur\n",
    "            erreur /= nb_entrainements\n",
    "            liste_erreur.append(erreur)\n",
    "            print(\"Erreur : \", erreur)\n",
    "\n",
    "            prec_test, err_test = precision_erreur(self, entree_t, sortie_t)\n",
    "            liste_erreur_t.append(err_test)\n",
    "            print(\"Erreur sur la base de test :\", liste_erreur_t[-1])\n",
    "\n",
    "            precision_e.append(succes_e / tot_e)\n",
    "            precision_t.append(prec_test)\n",
    "            print(\"Précision sur la base entraînement :\", precision_e[-1])\n",
    "            print(\"Précision sur la base test :\", precision_t[-1])\n",
    "            print(\"Durée de l'itération :\", round(time.time() - titer, 2), \"s\")\n",
    "            print()\n",
    "\n",
    "        print(\"Fin de l'apprentissage\")\n",
    "        print(\"Durée de l'apprentissage : \", round(time.time() - tini, 2), \"s\")\n",
    "        return (liste_erreur, liste_erreur_t, precision_e, precision_t)\n",
    "    \n",
    "    def test(self, entree_t, sortie_t):\n",
    "        nb_entrainements = len(entree_t)\n",
    "        erreur = 0\n",
    "        \n",
    "        for i in range(nb_entrainements):\n",
    "            # Propagation directe\n",
    "            sortie = entree_t[i]\n",
    "            for couche in self.couches:\n",
    "                sortie = couche.propagation_directe(sortie)\n",
    "            \n",
    "            # Ajout de l'erreur\n",
    "            erreur += self.erreur(sortie_t[i], sortie)\n",
    "        \n",
    "        erreur /= nb_entrainements\n",
    "        return erreur\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici quelques applications de ce code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XOR (ou exclusif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données (entree_e et sortie_e : liste des entrées/sorties voulues)\n",
    "entree_e = [np.reshape([0,0], (2,1)), np.reshape([0,1], (2,1)), \n",
    "            np.reshape([1,0], (2,1)), np.reshape([1,1], (2,1))]\n",
    "sortie_e = [np.array([0]), np.array([1]), np.array([1]), np.array([0])]\n",
    "\n",
    "# Structure du réseau (Reseau(couches, erreur, erreur_derivee))\n",
    "couches = [Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()]\n",
    "reseau1 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau2 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau3 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "\n",
    "# Entraînement (reseau.entrainement(entree_e, sortie_e, iterations, pas_apprentissage))\n",
    "err_pas1, _, _, _ = reseau1.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.01)\n",
    "err_pas2, _, _, _ = reseau2.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "err_pas3, _, _, _ = reseau3.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 1)\n",
    "\n",
    "# Visualisation des erreurs\n",
    "x = [x for x in range(1, 101)]\n",
    "plt.plot(x, err_pas1)\n",
    "plt.plot(x, err_pas2)\n",
    "plt.plot(x, err_pas3)\n",
    "plt.legend([\"0.01\", \"0.1\", \"1\"])\n",
    "plt.savefig(\"XOR : pas d'apprentissage 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couches = [Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()]\n",
    "\n",
    "reseau_al1 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau_al2 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau_al3 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau_al4 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "reseau_al5 = Reseau([Dense(2,5), Tanh(), Dense(5,3), Tanh(), Dense(3,1), Tanh()], eqm, eqm_derivee)\n",
    "\n",
    "err_alea1, _, _, _ = reseau_al1.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "err_alea2, _, _, _ = reseau_al2.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "err_alea3, _, _, _ = reseau_al3.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "err_alea4, _, _, _ = reseau_al4.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "err_alea5, _, _, _ = reseau_al5.entrainement(entree_e, sortie_e, entree_e, sortie_e, 100, 0.1)\n",
    "\n",
    "x = [x for x in range(1, 101)]\n",
    "plt.plot(x, err_alea1)\n",
    "plt.plot(x, err_alea2)\n",
    "plt.plot(x, err_alea3)\n",
    "plt.plot(x, err_alea4)\n",
    "plt.plot(x, err_alea5)\n",
    "plt.savefig(\"XOR : initialisation des poids et biais 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 MNIST (classification de chiffres écrits à la main)\n",
    "\n",
    "## 3.1 Pour que cela soit plus maniable, dans un premier temps, commençons par classer deux chiffres quelconques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist  # Données du MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la donnée\n",
    "(entree_e, sortie_e), (entree_t, sortie_t) = mnist.load_data()  # entree_t et sortie_t : Entrée et sortie test\n",
    "\n",
    "# Quelques données\n",
    "print(len(entree_e))\n",
    "print(len(entree_t))\n",
    "compte_e = [0] * 10\n",
    "compte_t = [0] * 10\n",
    "abcs = [x for x in range(10)]\n",
    "\n",
    "for x in sortie_e:\n",
    "    for i in range(10):\n",
    "        if x == i:\n",
    "            compte_e[i] += 1\n",
    "\n",
    "for x in sortie_t:\n",
    "    for i in range(10):\n",
    "        if x == i:\n",
    "            compte_t[i] += 1\n",
    "\n",
    "# Traitement de la donnée\n",
    "\n",
    "def nettoyage_mnist(chiffre1, chiffre2, entree, sortie, limite):\n",
    "    indices_1 = np.where(sortie == chiffre1)[0][:limite]  # Indices du chiffre 1, avec au maximum limite indices\n",
    "    indices_2 = np.where(sortie == chiffre2)[0][:limite]\n",
    "    indices = np.concatenate((indices_1, indices_2))\n",
    "    indices = np.random.permutation(indices)\n",
    "\n",
    "    x = entree[indices]\n",
    "    y = sortie[indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)  # Les images ont pour format (28,28) mais notre réseau prend en entrée une image de format (profondeur, hauteur, largeur)\n",
    "    x = x.astype(\"float32\") / 255  # x est de type uint8 et contient des entiers de 0 à 255\n",
    "\n",
    "    vecteur1 = np.reshape(np.array([1, 0]), (2, 1))\n",
    "    vecteur2 = np.reshape(np.array([0, 1]), (2, 1))\n",
    "    l = []\n",
    "    for i in y:\n",
    "        if i == chiffre1:\n",
    "            l.append(vecteur1)\n",
    "        else:\n",
    "            l.append(vecteur2)\n",
    "    y = np.array(l)\n",
    "    return (x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(entree_e, sortie_e) = nettoyage_mnist(0, 1, entree_e, sortie_e, 1000)\n",
    "(entree_t, sortie_t) = nettoyage_mnist(0, 1, entree_t, sortie_t, -1)\n",
    "\n",
    "print(entree_e.shape)\n",
    "print(sortie_e.shape)\n",
    "print(entree_t.shape)\n",
    "print(sortie_t.shape)\n",
    "\n",
    "plt.bar(abcs, compte_e)\n",
    "plt.savefig('mnist_compte_e', transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.bar(abcs, compte_t)\n",
    "plt.savefig('mnist_compte_t', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Version complète avec 10 chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réseau de neurones\n",
    "# Rappelons le format des couches utilisées:\n",
    "\"\"\"\n",
    "Dense(taille_entree, taille_sortie)\n",
    "Convolution(dimensions_entree, taille_filtre, profondeur_sortie)\n",
    "Dimension(dimensions_entree, dimension_sortie)\n",
    "\"\"\"\n",
    "\n",
    "couches = [\n",
    "    Convolution((1,28,28), 3, 5),\n",
    "    Tanh(),\n",
    "    Dimension((5,26,26), (5*26*26, 1)),\n",
    "    Dense(5*26*26, 100),\n",
    "    Tanh(),\n",
    "    Dense(100, 2),\n",
    "    Sigmoide()\n",
    "]\n",
    "\n",
    "reseau = Reseau(couches, bce, bce_derivee)\n",
    "(erreur_e, erreur_t, precision_e, precision_t) = reseau.entrainement(entree_e, sortie_e, entree_t, sortie_t, 10, 0.1)\n",
    "\n",
    "abcs = [x for x in range(1, 11)]\n",
    "plt.figure()\n",
    "plt.plot(abcs, erreur_e)\n",
    "plt.plot(abcs, erreur_t)\n",
    "plt.savefig(\"Mnist 2 chiffres : erreur\", transparent=True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(abcs, precision_e)\n",
    "plt.plot(abcs, precision_t)\n",
    "plt.savefig(\"Mnist 2 chiffres : précision\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. HAM10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Exploration de la base de donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "types = ['bkl', 'akiec', 'bcc', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "types_explicite = [\n",
    "    'Lésions bénignes comme la kératose',\n",
    "    'Kératose actinique/ carcinome intraépithélial ou maladie de Bowen',\n",
    "    'Carcinome basocellulaire',\n",
    "    'Dermatofibrome',\n",
    "    'Mélanome',\n",
    "    'Naevus mélanocytaire',\n",
    "    'Lésions vasculaires'\n",
    "]\n",
    "\n",
    "types_reduits = [\n",
    "    'Lésions bénignes',\n",
    "    'Kératose/ carcinome/ Bowen',\n",
    "    'Carcinome basocellulaire',\n",
    "    'Dermatofibrome',\n",
    "    'Mélanome',\n",
    "    'Naevus mélanocytaire',\n",
    "    'Lésions vasculaires'\n",
    "]\n",
    "\n",
    "indices = []\n",
    "\n",
    "for x in types:\n",
    "    indices.append(np.where(ham['dx'] == x)[0])\n",
    "\n",
    "compte = []\n",
    "for x in indices:\n",
    "    compte.append((len(x)) / 10015)\n",
    "\n",
    "plt.figure(figsize=(19,10))\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.bar([x for x in range(7)], compte)\n",
    "\n",
    "plt.savefig('Répartition des lésions dans la base de donnée pourcentage', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "loca_lesions = []\n",
    "for x in ham['dx_type']:\n",
    "    if x == 'histo':\n",
    "        loca_lesions.append('Histopathologie')\n",
    "    elif x == 'follow_up':\n",
    "        loca_lesions.append('Examen de suivi')\n",
    "    elif x == 'consensus':\n",
    "        loca_lesions.append(\"Consensus d'experts\")\n",
    "    elif x == 'confocal':\n",
    "        loca_lesions.append(\"Microscopie confocale in vivo\")\n",
    "\n",
    "loca_lesions = pd.Series(loca_lesions)\n",
    "plt.xticks(fontsize=12)\n",
    "loca_lesions.value_counts().plot(kind='bar')\n",
    "plt.savefig('Type de détermination du type des lésions', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Localisation de la lésion\", fontsize=17)\n",
    "ham['localization'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ham['age'].hist(bins=50)\n",
    "plt.savefig('Nombre de lésions par âge', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham['sex'].value_counts().plot(kind='bar')\n",
    "plt.savefig('Lésions par sexe', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"dx\", y=\"age\", hue=\"sex\", kind=\"bar\", data=ham)\n",
    "plt.savefig('Lésions par âge et sexe', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"dx\", kind=\"count\", hue=\"age\", palette='tab10', data=ham, height=4, aspect=3)\n",
    "plt.setp(g._legend.get_texts(), fontsize=10)\n",
    "g.set_xlabels(\"Type de lésion\", fontsize=10)\n",
    "g.set_ylabels(\"Nombre d'occurrences\", fontsize=10)\n",
    "g._legend.set_title('Âge')\n",
    "plt.savefig('Occurrence du nombre de lésions par sexe et type de lésion', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"localization\", kind=\"count\", hue=\"dx\", data=ham, palette=\"tab10\", height=5, aspect=3)\n",
    "g.set_xlabels(\"Localisation\", fontsize=10)\n",
    "g.set_ylabels(\"Nombre d'occurrence\", fontsize=10)\n",
    "g._legend.set_title('Type de lésions')\n",
    "plt.savefig('Type de lésions', transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Importation des images et implémentation du réseau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, l’utilisation des techniques précédentes est trop lent pour aboutir avec ce jeu de donnée. En effet, une itération prend à peu près 1 jour. C’est pourquoi nous allons faire le modèle sur Keras, en n’utilisant que des couches, fonctions d’erreurs et structures déjà codées auparavant. Le résultat est alors le même, mais beaucoup plus rapide, ce qui permet de faire plus d’itéations, et de comparer différents modèles.\n",
    "\n",
    "\n",
    "Néanmoins, l’importation des données diffère sur 2 points entre Keras et le réseau codé auparavant. En effet, Keras prend des images de la forme (hauteur, largeur, profondeur) au lieu de (profondeur, hauteur, largeur). L’importation d’une image sur python se faisant dans le format (hauteur, largeur, profondeur), il faut appliquer une fonction convertir (ci-dessous) à chaque image de la base de donnée pour le mettre en entrée du réseau codé auparavant. De plus, la sortie est au format (nombre_de_classe,) sur Keras au lieu (nombre_de_classe,1). Il suﬀit alors, au début du code, d’écrire identite = np.reshape(identite, (7,7,1)), à la suite de sa définition, si l’on souhaite utiliser cette base de donnée au réseau de neurone codé précedemment.\n",
    "\n",
    "Tout le reste étant similaire, implémentons le réseau sur Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Une première implémentation naïve : déséquilibre de classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction utile\n",
    "\n",
    "def convertir(im): # Convertir (x,y,3) en (3,x,y) a = im[:,:,0]\n",
    "    b = im[:,:,1]\n",
    "    c = im[:,:,2]\n",
    "    res = np.array([a,b,c]) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de la donnée\n",
    "types = ['bkl', 'akiec', 'bcc', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "# Matrice identité 7 (utile pour la sortie sous forme [1,0,0,0,0,0,0], [0,1,...], etc)\n",
    "identite = np.eye(7)\n",
    "\n",
    "# Indices pour chaque type de lésion\n",
    "indices = [] \n",
    "for x in types:\n",
    "    indices.append(np.where(ham['dx'] == x)[0].tolist()) \n",
    "\n",
    "# Séparation de la base de donnée en test et entrainement\n",
    "indices_test = []\n",
    "indices_entrainement = []\n",
    "\n",
    "for ind in indices: \n",
    "    k = 0\n",
    "    seuil = 4 * len(ind) / 5 \n",
    "    indices_test.append([]) \n",
    "    indices_entrainement.append([]) \n",
    "    for i in ind:\n",
    "        if k <= seuil: \n",
    "            indices_entrainement[-1].append(i) \n",
    "            k += 1\n",
    "        else: \n",
    "            indices_test[-1].append(i) \n",
    "            k += 1\n",
    "\n",
    "# On applatit les listes, et on mélange aléatoirement les indices\n",
    "def applatir(li): \n",
    "    res = []\n",
    "    for x in li: \n",
    "        res.extend(x)\n",
    "    return res\n",
    "\n",
    "indices_test = np.random.permutation(applatir(indices_test))\n",
    "indices_entrainement = np.random.permutation(applatir(indices_entrainement))\n",
    "\n",
    "# Enfin, on crée les listes entree_e, sortie_e, ...\n",
    "entree_e, sortie_e, entree_t, sortie_t = [], [], [], []\n",
    "k = 0\n",
    "\n",
    "for i in indices_test:\n",
    "    url = \"enter_ham_url\"\n",
    "    x = url + ham['image_id'][i] + '.jpg'\n",
    "    entree_t.append(np.asarray(Image.open(x).resize((64, 64))) / 255.)\n",
    "    sortie_t.append(ham['dx'][i])\n",
    "\n",
    "for i in indices_entrainement:\n",
    "    x = url + ham['image_id'][i] + '.jpg'\n",
    "    entree_e.append(np.asarray(Image.open(x).resize((64, 64))) / 255.)\n",
    "    sortie_e.append(ham['dx'][i])\n",
    "\n",
    "# On convertit les sorties sous formes de chaînes de caractères en [1,0,0,0,0,0,0] ou [0,1,...] etc\n",
    "s_e, s_t = [], []\n",
    "for x in sortie_e:\n",
    "    for i in range(7):\n",
    "        if x == types[i]: \n",
    "            s_e.append(identite[i])\n",
    "\n",
    "for x in sortie_t:\n",
    "    for i in range(7):\n",
    "        if x == types[i]: \n",
    "            s_t.append(identite[i])\n",
    "    \n",
    "# On convertit les listes en array\n",
    "sortie_e = np.asarray(s_e)\n",
    "entree_e = np.asarray(entree_e)\n",
    "sortie_t = np.asarray(s_t)\n",
    "entree_t = np.asarray(entree_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importations de fonctions et couches déjà codées, grâce à Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential  # Fonctionnement analogue à celui de Reseau\n",
    "from tensorflow.keras.layers import Conv2D  # Fonctionnement analogue à celui de Convolution\n",
    "from tensorflow.keras.layers import MaxPool2D  # Fonctionnement analogue à celui de Maxpooling\n",
    "from tensorflow.keras.layers import Dense  # Fonctionnement analogue à celui de Dense\n",
    "from tensorflow.keras.layers import Flatten  # Fonctionnement analogue à celui de Dimension (ou de l'usage qui en est fait en tout cas)\n",
    "from tensorflow.keras.layers import Dropout  # Fonctionnement analogue à celui de Dropout\n",
    "from tensorflow.keras.optimizers import SGD  # (Rétropropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_naif = Sequential()\n",
    "modele_naif.add(Conv2D(256, (3,3), activation = \"relu\", input_shape = (64,64,3)))\n",
    "modele_naif.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele_naif.add(Dropout(0.3))\n",
    "modele_naif.add(Conv2D(128, (3,3), activation = \"relu\"))\n",
    "modele_naif.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele_naif.add(Dropout(0.3))\n",
    "modele_naif.add(Conv2D(64, (3,3), activation = \"relu\"))\n",
    "modele_naif.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele_naif.add(Dropout(0.3))\n",
    "modele_naif.add(Flatten())\n",
    "modele_naif.add(Dense(32, activation = \"relu\"))\n",
    "modele_naif.add(Dense(7, activation = \"softmax\"))\n",
    "modele_naif.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descente_gradient = SGD(learning_rate=0.01)\n",
    "modele_naif.compile(optimizer = descente_gradient, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "repetitions = 50\n",
    "historique = modele_naif.fit(entree_e, sortie_e, epochs = repetitions, validation_data = (entree_t, sortie_t), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour information, voici le code équivalent sur la classe Réseau (testé mais trop lent)\n",
    "couches = [\n",
    "    Convolution((3,32,32), 3, 256),\n",
    "    Relu(),\n",
    "    Maxpooling((256,30,30), (2,2), 2),\n",
    "    Relu(),\n",
    "    Dropout(0.3),\n",
    "    Convolution((256,15,15), 3, 128),\n",
    "    Relu(),\n",
    "    Maxpooling((128,13,13), (2,2), 2),\n",
    "    Dropout(0.3),\n",
    "    Dimension((128,6,6), (128*6*6,1)),\n",
    "    Dense(128*6*6, 32),\n",
    "    Relu(),\n",
    "    Dense(32, 7),\n",
    "    Softmax()\n",
    "]\n",
    "reseau_naif_2 = Reseau(couches, cce, cce_derivee)\n",
    "erreur, precision_e, precision_t = reseau_ham.entrainement(entree_e, sortie_e, entree_t, sortie_t, repetitions, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement de la donnée\n",
    "types = ['bkl', 'akiec', 'bcc', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "# Matrice identité 7 (utile pour la sortie sous forme [1,0,0,0,0,0,0],[0,1,...], etc)\n",
    "identite = np.eye(7)\n",
    "\n",
    "# Indices pour chaque type de lésion\n",
    "indices = [] \n",
    "for x in types:\n",
    "    indices.append(np.where(ham['dx'] == x)[0].tolist())\n",
    "\n",
    "# Séparation de la base de donnée en test et entrainement\n",
    "indices_test = []\n",
    "indices_entrainement = []\n",
    "for ind in indices:\n",
    "    k = 0\n",
    "    seuil = 4 * len(ind) / 5\n",
    "    indices_test.append([]) \n",
    "    indices_entrainement.append([])\n",
    "    for i in ind:\n",
    "        if k <= seuil:\n",
    "            indices_entrainement[-1].append(i)\n",
    "            k += 1\n",
    "        else:\n",
    "            indices_test[-1].append(i)\n",
    "            k += 1\n",
    "\n",
    "# NOUVEAUTE : Compte du nombre de donnée de test et d'entraînement pour chaque classe\n",
    "compte_test = []\n",
    "compte_entrainement = []\n",
    "for x in indices_test:\n",
    "    compte_test.append(len(x))\n",
    "for x in indices_entrainement:\n",
    "    compte_entrainement.append(len(x))\n",
    "\n",
    "# NOUVEAUTE : On augmente/diminue le nombre d'indices par classe\n",
    "courant_t = [0] * 7\n",
    "courant_e = [0] * 7\n",
    "for i in range(7):\n",
    "    while len(indices_test[i]) >= 101:\n",
    "        (indices_test[i]).pop()\n",
    "    while len(indices_test[i]) <= 99:\n",
    "        (indices_test[i]).append(indices_test[i][courant_t[i]])\n",
    "        courant_t[i] = (courant_t[i] + 1) % compte_test[i]\n",
    "\n",
    "for i in range(7):\n",
    "    while len(indices_entrainement[i]) >= 501:\n",
    "        (indices_entrainement[i]).pop()\n",
    "    while len(indices_entrainement[i]) <= 499:\n",
    "        (indices_entrainement[i]).append(indices_entrainement[i][courant_e[i]])\n",
    "        courant_e[i] = (courant_e[i] + 1) % compte_entrainement[i]\n",
    "\n",
    "# On applatit les listes, et on mélange aléatoirement les indices\n",
    "def applatir(li): \n",
    "    res = []\n",
    "    for x in li: \n",
    "        res.extend(x)\n",
    "    return res\n",
    "\n",
    "indices_test = np.random.permutation(applatir(indices_test))\n",
    "indices_entrainement = np.random.permutation(applatir(indices_entrainement))\n",
    "\n",
    "# Enfin, on crée les listes entree_e, sortie_e, ...\n",
    "entree_e, sortie_e, entree_t, sortie_t = [], [], [], []\n",
    "k = 0\n",
    "for i in indices_test:\n",
    "    x = \"/Users/yassinelaraki/Desktop/TIPE RE/dataverse_files/HAM10000/\" + ham['image_id'][i] + '.jpg'\n",
    "    entree_t.append(np.asarray(Image.open(x).resize((32,32))) / 255.)\n",
    "    sortie_t.append(ham['dx'][i])\n",
    "\n",
    "for i in indices_entrainement:\n",
    "    x = \"/Users/yassinelaraki/Desktop/TIPE RE/dataverse_files/HAM10000/\" + ham['image_id'][i] + '.jpg'\n",
    "    entree_e.append(np.asarray(Image.open(x).resize((32,32))) / 255.)\n",
    "    sortie_e.append(ham['dx'][i])\n",
    "\n",
    "# On convertit les sorties sous formes de chaînes de caractères en [1,0,0,0,0,0,0] ou [0,1,...] etc\n",
    "s_e, s_t = [], []\n",
    "for x in sortie_e:\n",
    "    for i in range(7):\n",
    "        if x == types[i]:\n",
    "            s_e.append(identite[i])\n",
    "\n",
    "for x in sortie_t:\n",
    "    for i in range(7):\n",
    "        if x == types[i]:\n",
    "            s_t.append(identite[i])\n",
    "\n",
    "# On convertit les listes en array\n",
    "sortie_e = np.asarray(s_e)\n",
    "entree_e = np.asarray(entree_e)\n",
    "sortie_t = np.asarray(s_t)\n",
    "entree_t = np.asarray(entree_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele = Sequential()\n",
    "\n",
    "modele.add(Conv2D(256, (3,3), activation = \"relu\", input_shape = (64,64,3)))\n",
    "modele.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele.add(Dropout(0.3))\n",
    "modele.add(Conv2D(128, (3,3), activation = \"relu\"))\n",
    "modele.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele.add(Dropout(0.3))\n",
    "modele.add(Conv2D(64, (3,3), activation = \"relu\"))\n",
    "modele.add(MaxPool2D(pool_size = (2,2)))\n",
    "modele.add(Dropout(0.3))\n",
    "modele.add(Flatten())\n",
    "modele.add(Dense(32, activation = \"relu\"))\n",
    "modele.add(Dense(7, activation = \"softmax\"))\n",
    "\n",
    "modele.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Une solution : même nombre d’images par classe, en détruisant ou en répliquant des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descente_gradient = SGD(learning_rate=0.01)\n",
    "modele_naif.compile(optimizer = descente_gradient, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "repetitions = 50\n",
    "historique = modele_naif.fit(entree_e, sortie_e, epochs = repetitions, \n",
    "                             validation_data = (entree_t, sortie_t), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 Test du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "\n",
    "# Erreur (loss = erreur, val_loss = erreur_test)\n",
    "erreur = historique.history['loss']\n",
    "erreur_test = historique.history['val_loss']\n",
    "\n",
    "repetitions = range(1, len(erreur) + 1)\n",
    "plt.plot(repetitions, erreur, 'y', label='Erreur entrainement')\n",
    "plt.plot(repetitions, erreur_test, 'r', label='Erreur test')\n",
    "plt.xlabel('Répétitions')\n",
    "plt.ylabel('Erreur')\n",
    "plt.savefig('Erreur du HAM10000 essai 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Précision\n",
    "precision = historique.history['acc']\n",
    "precision_test = historique.history['val_acc']\n",
    "plt.plot(repetitions, precision, 'y', label='Précision entrainement')\n",
    "plt.plot(repetitions, precision_test, 'r', label='Précision test')\n",
    "plt.xlabel('Répétitions')\n",
    "plt.ylabel('Précision')\n",
    "plt.savefig('Précision du HAM10000 essai 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Prédictions faites sur chaque échantillon\n",
    "prediction_test = model.predict(entree_t)\n",
    "classes_test = np.argmax(prediction_test, axis=1)\n",
    "vraies = np.argmax(sortie_t, axis=1)\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(vraies, classes_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.set(font_scale=1.6)\n",
    "sns.heatmap(cm, annot=True, linewidths=.5, ax=ax)\n",
    "plt.savefig('Matrice de confusion essai 2')\n",
    "plt.show()\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Pourcentage de prédictions erronées\n",
    "faux_pourcentage = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
    "plt.bar(np.arange(7), faux_pourcentage)\n",
    "\n",
    "plt.xlabel('Vraie classe')\n",
    "plt.ylabel('Pourcentage de prédictions incorrectes')\n",
    "plt.savefig('Graph precisions du HAM essai 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
